<!DOCTYPE html>

<html>
<meta charset="utf-8" />
<!--remember to include the following for mobile-->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<head>
    <title>Supervised Learning | CZ4041 | Jason Ngo's website</title>
    <link rel="icon" href="../../../../../images/sciurus_96.png">
    <link rel="stylesheet" type="text/css"
        href="../../../../../styles/default_style.css">
</head>

<body>
    <h2 class="header">
        CZ4041 Machine Learning
    </h2>

    <!--start of nav-related code-->
    <ul class="navbar">
        <li>
            <a href="../../../../../index.html">
                home
            </a>
        </li>

        <li>
            <a href="../../../../main.html">
                education
            </a>
        </li>

        <li>
            <a href="../../../../../experience.html">
                internship
            </a>
        </li>

        <li>
            <a href="../../../../../miscellaneous/main.html">
                miscellaneous
            </a>
        </li>
    </ul>
    <!--end of nav-related code-->

    <!--start of content-related code-->
    <div class="content">
        <h3>
            Supervised Learning
        </h3>

        <p>
            In supervised learning, the examples presented to the computer are
            pairs of inputs and corresponding outputs (labelled training data).
            The goal is to learn a model that maps inputs to labels.
        </p>

        <h4>in mathematics</h4>

        <p>
            given - a set of {<i>x<sub>i</sub></i>, <i>y<sub>i</sub></i>} for
            <i>i</i> = 1, 2, ..., <i>N</i>,
            where:
            <ul>
                <li>
                    <i>x<sub>i</sub></i> = [<i>x</i><sub><i>i</i>1</sub>,
                    <i>x</i><sub><i>i</i>2</sub>, ..., <i>x<sub>im</sub></i>] (a
                    vector of features - m is number of features)
                </li>

                <li>
                    <i>y<sub>i</sub></i> is a scalar (the output)
                </li>

                <li>
                    <i>N</i> is number of labelled training data
                </li>
            </ul>
        </p>

        <p>
            goal - learn a mapping <i>f</i>:<i>x</i> &rightarrow; <i>y</i> by
            requiring <i>f</i>(<i>x<sub>i</sub></i>) = <i>y<sub>i</sub></i>
        </p>

        <p>
            The learnt mapping <i>f</i> is expected to be able to make precise
            predictions on any unseen <i>x*</i> as <i>f</i>(<i>x*</i>).
        </p>

        <h4>examples</h4>

        <p>TODO</p>

        <h4>classification vs regression</h4>

        <p>
            For classification, <i>y</i> is discrete.

            <ul>
                <li>
                    If <i>y</i> is binary, then classification is binary
                    classification.
                </li>

                <li>
                    If <i>y</i> is nominal instead of binary, then
                    classification is multi-class classification.
                </li>
            </ul>
        </p>

        <p>
            For regression, <i>y</i> is continuous.
        </p>

        <p>
            If an example is assigned to more than one category, then
            classification is multi-label classification.

            <ul>
                <li>
                    Each instance can have more than 1 label.
                </li>

                <li>
                    For binary classification and multi-class classification,
                    each instance is mapped to only 1 class.
                </li>
            </ul>
        </p>

        <h4>typical learning procedure</h4>

        <p>Consists of 2 phases.</p>

        <p>
            training phase:<br>
            Given labelled training data {<i>x<sub>i</sub></i>,
            <i>y<sub>i</sub></i>}, for <i>i</i> = 1, 2, ..., <i>N</i>, apply
            supervised learning algorithms on {<i>x<sub>i</sub></i>,
            <i>y<sub>i</sub></i>} to learn a model <i>f</i> such that
            <i>f</i>(<i>x<sub>i</sub></i>) = <i>y<sub>i</sub></i>.
        </p>

        <p>
            testing phase:<br>
            Given unseen test data <i>x<sub>i</sub>*</i>, for <i>i</i> = 1, 2,
            ..., <i>T</i>, use the trained model <i>f</i> to make predictions
            <i>f</i>(<i>x<sub>i</sub>*</i>)
        </p>

        <h4>2 ways of learning</h4>

        <p>
            inductive learning: classifier is induced

            <ul>
                <li>
                    during training, feed examples to classification algorithm
                    to obtain a classifier
                </li>

                <li>
                    during testing, simply pass the input through the classifier
                    to obtain the predicted output
                </li>

                <li>
                    slow during training phase, fast during testing phase
                </li>
            </ul>
        </p>

        <p>
            lazy learning: no classifier is learnt

            <ul>
                <li>
                    example: <i>K</i> nearest neighbour
                </li>

                <li>
                    during training, simply store each example
                </li>

                <li>
                    during testing, search through entire database of examples,
                    find the example most similar to input datum, then output
                    the label of that example (i.e. pattern matching)
                </li>

                <li>
                    fast during training phase, slow during testing phase
                </li>

                <li>
                    not very useful for practical purposes - need output in
                    real-time
                </li>
            </ul>
        </p>

        <h4>ways to learn a predictive model (inductive learning)</h4>

        <p>
            <ul>
                <li>
                    Bayesian classifiers
                </li>

                <li>
                    decision trees
                </li>

                <li>
                    artificial neural networks
                </li>

                <li>
                    support vector machines
                </li>

                <li>
                    regularised regression model
                </li>
            </ul>
        </p>

        <p>TODO: examples</p>

        <h4>evaluation of supervised learning</h4>

        <p>
            The learnt predictive model should be able to make predictions on
            previously unseen data as accurately as possible.
        </p>

        <p>
            The solution is to divide the whole training data into 'training'
            and 'test' sets, with 'training' set used to learn the predictive
            model and 'test' set used to validate the performance of the learnt
            model.
        </p>

        <h4>common evaluation metrics</h4>

        <p>
            classification:

            <ul>
                <li>
                    accuracy - ratio of correct predictions to total test data
                </li>

                <li>
                    error rate - ratio of incorrect predictions to total test
                    data (error rate = 1 - accuracy)
                </li>
            </ul>
        </p>

        <p>
            regression:

            <ul>
                <li>
                    mean absolute error
                </li>

                <li>
                    root mean squared error
                </li>
            </ul>
        </p>

        <h4>split of training and evaluation sets</h4>

        <p>
            random subsampling - sample <b>without</b> replacement <i>k</i>
            number of
            times

            <ul>
                <li>
                    sum_accuracy = 0<br>
                    # k is number of times to do random subsampling<br>
                    for i in range(k):<br>
                    &nbsp; # N is number of examples in original training
                    set<br>
                    &nbsp; # split_ratio is ratio of data in new training set to
                    original training set (e.g. &frac23;)<br>
                    &nbsp; for j in range(split_ratio * N):
                    <br>
                    &nbsp; &nbsp; <b>move</b>_a_random_example_from_original_to_new_training_set()<br>
                    &nbsp; transfer_the_rest_of_original_to_validation_set()<br>
                    &nbsp; accuracy = train_model(train_data, val_data)<br>
                    &nbsp; sum_accuracy += accuracy<br>
                    &nbsp; average_accuracy = sum_accuracy / k
                </li>

                <li>
                    average accuracy = <sup>1</sup>/<sub><i>k</i></sub>
                    <sup><i>k</i></sup>&sum;<sub><i>i</i>=1</sub>
                    accuracy<sub>i</sub>
                </li>
            </ul>
        </p>

        <p>
            cross validation:
            <br><br>

            <i>k</i>-fold cross validation - partition data into <i>k</i>
            subsets of the same size

            <ul>
                <li>
                    hold one group aside for testing and use the rest to build
                    model
                </li>

                <li>
                    repeat <i>k</i> number of times with each subset being used
                    for validation once
                </li>

                <li>
                    usually, <i>k</i> = 3 or 5
                </li>
            </ul>

            leave-one-out - special case of <i>k</i>-fold cross validation

            <ul>
                <li>
                    <i>k</i> = <i>N</i>, where <i>N</i> is number of data
                    instances for training
                </li>

                <li>
                    was popular in the past when datasets were small
                </li>

                <li>
                    no longer practical due to big data
                </li>
            </ul>
        </p>

        <p>
            bootstrap - sampled with replacement

            <ul>
                <li>
                    same as random subsampling, except that there is replacement
                </li>

                <li>
                    sum_accuracy = 0<br>
                    # k is number of times to do random subsampling<br>
                    for i in range(k):<br>
                    &nbsp; # N is number of examples in original training
                    set<br>
                    &nbsp; # split_ratio is ratio of data in new training set to
                    original training set (e.g. &frac23;)<br>
                    &nbsp; for j in range(split_ratio * N):
                    <br>
                    &nbsp; &nbsp; <b>copy</b>_a_random_example_from_original_to_new_training_set()<br>
                    &nbsp; transfer_the_rest_of_original_to_validation_set()<br>
                    &nbsp; accuracy = train_model(train_data, val_data)<br>
                    &nbsp; sum_accuracy += accuracy<br>
                    &nbsp; average_accuracy = sum_accuracy / k
                </li>
            </ul>
        </p>
    </div>
    <!--end of content-related code-->

    <!--start of nav-buttons code-->
    <div class="nav-buttons">
        <a href="../lecture_0/introduction.html">
            &#8249; introduction
        </a>

        <a href="../../main.html">
            content
        </a>

        <a class="disabled">
            next &#8250;
        </a>
    </div>
    <!--end of nav-buttons code-->

    <!--start of footer-related code-->
    <div class="footer">
        <div class="github">
            <img src="../../../../../images/GitHub-Mark-120px-plus.png">

            <a target="_blank" href="https://github.com/NgoJunHaoJason">
                GitHub profile
            </a>
        </div>

        <div class="last-modified">
            last modified:
            <span id="last_modified">null</span>
        </div>
    </div>
    <!--end of footer-related code-->

    <script defer="defer" src="../../../../../scripts/on_page_load.js"></script>
</body>

</html>