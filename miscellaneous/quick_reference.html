<!DOCTYPE html>

<html>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<head>
    <title>quick reference | Jason Ngo's website</title>
    <link rel="icon" target="_blank" href="../images/sciurus_96.png">
    <link rel="stylesheet" type="text/css" target="_blank" href="../styles/default_style.css">
</head>

<body>
    <h2 class="header">
        Quick Reference
    </h2>

    <!--start of nav-related code-->
    <ul class="navbar">
        <li>
            <a href="../index.html">
                home
            </a>
        </li>

        <li>
            <a href="../education/main.html">
                education
            </a>
        </li>

        <li>
            <a href="../experience.html">
                experience
            </a>
        </li>

        <li>
            <a href="main.html">
                miscellaneous
            </a>
        </li>
    </ul>
    <!--end of nav-related code-->

    <!--start of content-related code-->
    <div class="content">
        <h3>AdaBoost</h3>
        <p>
            the main idea
            <ul>
                <li>
                    focus more on hard-to-detect objects in an image
                </li>

                <li>
                    use a bunch of weak classifiers and train them sequentially, with each classifier focusing on areas
                    missed by the previous weak classifier, finally aggregating them to get a strong classifier
                </li>
            </ul>

            note
            <ul>
                <li>
                    short for Adaptive Boosting
                </li>

                <li>
                    not really in use for computer vision anymore - deep learning CV > traditional CV
                </li>

                <li>
                    unsure of the term Pr<sub>i ~ D<sub>t</sub></sub> [...]
                </li>
            </ul>

            terms
            <ul>
                <li>
                    X - a set of training data
                </li>
                <li>
                    m - number of training data
                </li>
                <li>
                    y<sub>i</sub> - actual output of training datum x<sub>i</sub>
                </li>
                <li>
                    D - weights for training data
                </li>
                <li>
                    D<sub>t</sub> - weights for training data during the t<sup>th</sup> training iteration
                </li>
                <li>
                    D<sub>1</sub>(i) - weights for training datum i during the 1<sup>st</sup> training iteration
                </li>
                <li>
                    h<sub>t</sub> - chosen weak classifier at the t<sup>th</sup> training iteration
                </li>
                <li>
                    Pr - Probability
                </li>
                <li>
                    &epsilon;<sub>t</sub> - weighted error of the outputs of the chosen weak classifier h<sub>t</sub> at
                    the t<sup>th</sup> training iteration
                </li>
                <li>
                    &alpha;<sub>t</sub> - weight of the chosen classifier at the t<sup>th</sup> training iteration
                </li>
                <li>
                    Z<sub>t</sub> - a normalisation factor at the t<sup>th</sup> training iteration, chosen such that
                    D<sub>t+1</sub> is a distribution
                </li>
                <li>
                    H - strong classifier
                </li>
            </ul>

            algorithm
            <!--does not look very readable in html thanks to auto-format-->
            <ul>
                <li>
                    given: (x<sub>1</sub>, y<sub>1</sub>), ..., (x<sub>m</sub>, y<sub>m</sub>) where x<sub>i</sub>
                    &isin; X, y<sub>i</sub> &isin; {-1, 1}
                </li>
            </ul>
            <ol>
                <li>
                    for i = 1, ..., m:
                    <ul>
                        <li>
                            D<sub>1</sub>(i) = <sup>1</sup>&frasl;<sub>m</sub>
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    for t = 1, ..., T:
                    <ul>
                        <li>
                            train weak learner using distribution D
                        </li>
                        <li>
                            pick h<sub>t</sub>: X &RightArrow; {-1, 1} which minimises &epsilon;<sub>t</sub> = Pr<sub>i
                                ~ D<sub>t</sub></sub> [h<sub>t</sub>(x<sub>i</sub>) &ne; y<sub>i</sub>]
                        </li>
                        <li>
                            get &alpha;<sub>t</sub> = <sup>1</sup>&frasl;<sub>2</sub> ln(<sup>1 -
                                &epsilon;<sub>t</sub></sup> &frasl;
                            <sub>&epsilon;<sub>t</sub></sub>)
                        </li>
                        <li>
                            for i = 1, ..., m:
                            <ul>
                                <li>
                                    D<sub>t+1</sub>(i) = (<sup>D<sub>t</sub>(i)</sup> &frasl;
                                    <sub>Z<sub>t</sub></sub>)&middot;e<sup>-&alpha;<sub>t</sub>&middot;h<sub>t</sub>(x<sub>i</sub>)&middot;y<sub>i</sub></sup>
                                </li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <br>
                <li>
                    output H(x) = sign_of(&sum;<sub>t=1, ..., T</sub>(&alpha;<sub>t</sub>&middot;h<sub>t</sub>(x)))
                </li>
            </ol>

            references
            <ul>
                <li>
                    <a target="_blank"
                        href='https://towardsdatascience.com/boosting-and-adaboost-clearly-explained-856e21152d3e'>
                        Boosting and AdaBoost clearly explained
                    </a>
                </li>

                <li>
                    <a target="_blank"
                        href='https://towardsdatascience.com/adaboost-for-dummies-breaking-down-the-math-and-its-equations-into-simple-terms-87f439757dcf'>
                        Adaboost for Dummies: Breaking Down the Math (and its Equations) into Simple Terms
                    </a>
                </li>

                <li>
                    <a target="_blank" href='https://towardsdatascience.com/boosting-algorithm-adaboost-b6737a9ee60c'>
                        Boosting algorithm: AdaBoost
                    </a>
                </li>
            </ul>
        </p>

        <h3>Focal Loss</h3>
        <p>
            the main idea
            <ul>
                <li>
                    focus more on hard-to-detect objects in an image
                    <ul>
                        <li>
                            just like AdaBoost (different implementation though)
                        </li>
                    </ul>
                </li>

                <li>
                    TODO
                </li>
            </ul>

            note
            <ul>
                <li>
                    at &gamma; = 0, FL(p<sub>t</sub>) = CE(p<sub>t</sub>)
                </li>

                <li>
                    TODO
                </li>
            </ul>

            terms
            <ul>
                <li>
                    y &isin; {&plusmn;1} - groundtruth class: 1 for object detected, -1 for no object detected
                </li>

                <li>
                    p &isin; [0,1] - model's estimated probability for the class with label y = 1
                </li>

                <li>
                    CE(p, y) - cross entropy loss for binary classification: -log(p) if y = 1, -log(1-p) otherwise
                </li>

                <li>
                    p<sub>t</sub> - same as p, but rewritten for convenience: p if y = 1, (1-p) otherwise
                </li>

                <li>
                    CE(p<sub>t</sub>) - cross entropy loss for binary classification, rewritten for convenience: CE(p,
                    y) = -log(p<sub>t</sub>)
                </li>

                <li>
                    &gamma; &GreaterEqual; 0 - tunable focusing parameter that smoothly adjusts the rate at which easy
                    examples are down-weighted
                </li>

                <li>
                    (1 - p<sub>t</sub>)<sup>&gamma;</sup> - modulating factor
                </li>

                <li>
                    &alpha; - hyperparameter that weights the losses for differentclasses/background according to their
                    relative frequency
                </li>
            </ul>

            equations
            <ul>
                <li>
                    original version (binary classification): FL(p<sub>t</sub>) = &alpha;<sub>t</sub> * (1 -
                    p<sub>t</sub>)<sup>&gamma;</sup> * CE(p<sub>t</sub>)
                </li>
            </ul>

            references
            <ul>
                <li>
                    <a target="_blank" href="https://arxiv.org/abs/1708.02002">
                        Focal Loss for Dense Object Detection
                    </a>
                </li>

                <li>
                    <a target="_blank" href="https://blog.csdn.net/u014380165/article/details/77019084">
                        Focal Loss
                    </a>
                    (summary in Chinese)
                </li>

                <li>
                    <a target="_blank"
                        href="https://towardsdatascience.com/neural-networks-intuitions-3-focal-loss-for-dense-object-detection-paper-explanation-61bc0205114e">
                        Neural Networks Intuitions: 3. Focal Loss for Dense Object Detection — Paper Explanation
                    </a>
                </li>

                <li>
                    <a target="_blank" href="https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d">
                        The intuition behind RetinaNet
                    </a>
                </li>

                <li>
                    <a target="_blank"
                        href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4">
                        Review: RetinaNet — Focal Loss (Object Detection)
                    </a>
                </li>

                <li>
                    <a target="_blank"
                        href="https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets/">
                        Multi-class classification with focal loss for imbalanced datasets
                    </a>
                    (multi-class instead of binary; has code)
                </li>

                <li>
                    <a target="_blank" href="https://github.com/umbertogriffo/focal-loss-keras/issues/1">
                        About Alpha parameter in focal loss
                    </a>
                </li>
            </ul>
        </p>
    </div>
    <!--end of content-related code-->

    <!--start of nav-buttons code-->
    <div class="nav-buttons">
        <a href="main.html">
            back
        </a>
    </div>
    <!--end of nav-buttons code-->

    <!--start of footer-related code-->
    <div class="footer">
        <div class="github">
            <img src="../images/GitHub-Mark-120px-plus.png">

            <a target="_blank" href="https://github.com/NgoJunHaoJason">
                GitHub profile
            </a>
        </div>

        <div class="last-modified">
            last modified:
            <span id="last_modified">null</span>
        </div>
    </div>
    <!--end of footer-related code-->

    <script defer="defer" src="../scripts/on_page_load.js"></script>
</body>

</html>